{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Which airports have the largest number of departure delays?\n",
    "\n",
    "This Jupyter notebook contains executable Spark code written in Scala.\n",
    "\n",
    "If you want to try it out, then the first thing to do is to install [Jupyter](http://jupyter.org/) and the [Spark Kernel](https://github.com/ibm-et/spark-kernel) for it. The latter has to be built from source which takes a *very* long time. Follow the instructions in the project's wiki to install and integrate it with Jupyter.\n",
    "\n",
    "Let's get to work!\n",
    "\n",
    "Import the [dataset](http://stat-computing.org/dataexpo/2009/the-data.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark._\n",
    "\n",
    "val data = sc.textFile(\"2008.csv\").map(_.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next chunk is copied from [a StackOverflow answer](http://stackoverflow.com/questions/24299427/how-do-i-convert-csv-file-to-rdd) on how to remove csv headers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SimpleCSVHeader(header:Array[String]) extends Serializable {\n",
    "  val index = header.zipWithIndex.toMap\n",
    "  def apply(array:Array[String], key:String):String = array(index(key))\n",
    "}\n",
    "val header = new SimpleCSVHeader(data.take(1)(0))\n",
    "val filtered = data.filter(line => header(line,\"Origin\") != \"Origin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the required fields and determine if a flight is delayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "val delayed = filtered.map(data => (data(15), data(16))).filter(data => data._1 != \"NA\" && data._1.toInt > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate delays and materialize the top ten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val results = delayed.groupBy(_._2).map(data => (data._1, data._2.size)).sortBy(_._2, ascending = false).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 10 airports with the biggest number of delayed departures (with the number of delays) are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ATL,175017)\n",
      "(ORD,159427)\n",
      "(DFW,127749)\n",
      "(DEN,104414)\n",
      "(LAX,87258)\n",
      "(IAH,87139)\n",
      "(PHX,82915)\n",
      "(LAS,76240)\n",
      "(EWR,69612)\n",
      "(DTW,59837)\n"
     ]
    }
   ],
   "source": [
    "results.foreach(println)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.5.1",
   "language": "scala",
   "name": "spark"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
